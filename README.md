# accounting-bot
Å¾
This repository is used in the paper 

### Developing an accounting virtual assistant through Supervised Fine-Tuning (SFT) of a Small Language Model (SLM)" which is under review for the wiley publisher.

##### abstract

The development of an in-house accounting bot, an artificial intelligence (AI) assistant capable of generating internally structured bookkeeping double-entry posting schemes, is explored in this paper. The processes of curating a suitable dataset, selecting and fine-tuning a 7-billion-parameter language model, categorized as a Small Language Model (SLM)\footnote{SLMs typically refer to models with fewer than 10 billion parameters, whereas medium-sized models often have 14B parameters, and large-scale models exceed 70B.}, are described. A human-evaluated benchmark is also presented to assess model performance.

To achieve efficient Supervised Fine-Tuning (SFT), Low-Rank Adaptation (LoRA) was employed, significantly reducing memory requirements by using a small set of trainable parameters while maintaining model expressiveness. The process of back\-propagation was further optimized using Unsloth, a high-performance training framework for efficient video memory usage and flash attention mechanisms that accelerates adaptation and reduces memory overhead. The model which layers was updated is called QwenCoder2.5. It was selected with the presumption that it would be able to learn how to generate and examine bookkeeping patterns generated by accounting information system (AIS) in 17 years old history.

This proof of concept aims to support researchers and practitioners exploring the integration of generative AI in accounting by providing insights into both the benefits and challenges of AI-driven automation in bookkeeping tasks. The study demonstrates how an SLM can be fine-tuned on a proprietary dataset of journal posting schemes to assist accountants, auditors, and financial analysts while also facilitating synthetic data generation. Challenges related to AI, data preprocessing, fine-tuning optimization, and evaluation methodology are introduced and examined. 





### train.py
is the code for LoRA https://unsloth.ai/ finetuning qwencoder 7B language model on bookkeeping double journal entries from 2007-2023 database. You can find the constrained and anonymized version of database on the link https://huggingface.co/datasets/mariozupan/bookkeeping-posting-schemes-2007-2023. While the model described in a paper has been trained on unmasked and unconstrained dataset, this version has been used only for proofing the concept of the paper and understanding the concept which is reproducable.

### requirements.txt
represents neccessary libraries installed inside nvidia apptainer which runs on https://www.srce.unizg.hr/en/advanced-computing

### ollama Modelfile
is configuration file for inference with the model in ollama llama.cpp wrapper.

